<style>
    .image-container {
        position: relative;
        display: inline-block;
    }
    .label {
        position: absolute;
        top: 4px;
        left: 4px;
        background-color: rgba(0, 0, 255, 0.7); /* è“è‰²èƒŒæ™¯ï¼Œé€æ˜åº¦ä¸º0.7 */
        color: white;
        padding: 4px 8px;
        font-size: 12px;
        border-radius: 4px;
    }
    img {
        display: block;
    }
</style>


# ğŸ“ Publications 
<!-- åŠ ç‚¹è¡¨æƒ…åŒ…,ç›´æ¥å¤åˆ¶å›¾ç‰‡å³å¯  https://github.com/guodongxiaren/README/blob/master/emoji.md?tdsourcetag=s_pcqq_aiomsg -->

A full publication list is available on my [google scholar](https://scholar.google.com/citations?user=yuiXa5EAAAAJ&hl=en&oi=ao) page.

(*: equal contribution; â€ : corresponding authors.)


<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/videogen.jpg" width="350"> <div class="label">Video Generation</div></div>
</th><th style="text-align:left" width="70%"> <span style="font-size:18px">A Survey on Video Diffusion Models</span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">  ACM Computing Survey (<strong>CSUR, IF=23.8</strong>), 2024</span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2310.10647">Paper</a>][<a href="https://github.com/ChenHsing/Awesome-Video-Diffusion-Models">HomePage</a>][<a href="https://zhuanlan.zhihu.com/p/661860981">Zhihu</a>][<a href="https://mp.weixin.qq.com/s/qes6C8UbEYArnVKU3eu9QQ">æœºå™¨ä¹‹å¿ƒ</a>][<a href="https://mp.weixin.qq.com/s/viC_J08bVIVRzvRYxRyQTw">é‡å­ä½</a>]</span><br> <span style="color: red;"> Surveying 300+ recent literatures on video generation and editing with diffusion models. Acheving Github 2000+ stars.
</span>
</th>
</tr></tbody></table>


<table style="width:100%">
<tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/simda.jpg" width="350"> <div class="label">Video Generation</div></div>
</th><th style="text-align:left" width="70%"> 
<span style="font-size:18px">SimDA: A Simple Diffusion Adapter for Efficient Video Generation</span><br> 
<span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Han Hu, Zuxuan Wu, Yu-Gang Jiang</span></span><br>
 <span style="font-weight:normal;font-size:16px">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 </span><br> 
 <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2308.09710">Paper</a>][<a href="https://chenhsing.github.io/SimDA/">HomePage</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/stableanimator.jpg" width="350"> <div class="label">Video Generation</div></div>
</th><th style="text-align:left" width="70%"> <span style="font-size:18px">StableAnimator: High-Quality Identity-Preserving Human Image Animation
</span><br> <span style="font-weight:normal;font-size:16px">Shuyuan Tu, </span> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu</span></span><br> <span style="font-weight:normal;font-size:16px"> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025</span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2411.17697">Paper</a>][<a href="https://github.com/Francis-Rings/StableAnimator">Code</a>][<a href="https://francis-rings.github.io/StableAnimator/">Homepage</a>][<a href="https://mp.weixin.qq.com/s/qK3s-us2XeDv7phW83W5BQ">æœºå™¨ä¹‹å¿ƒ</a>]</span><br> <span style="color: red;"> Acheving Github 1200+ stars.
</span>
</th>
</tr></tbody></table>


<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/magicmotion.jpg" width="350"> <div class="label">Video Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance </span><br> <span style="font-weight:normal;font-size:16px">Quanhao Li*</span>, <span style="font-size:16px">Zhen Xing*<span style="font-weight:normal">, Rui Wang, Hui Zhang, Zuxuan Wu</span></span><br> <span style="font-weight:normal;font-size:16px">Technical Report, 2025 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2503.16421">Paper</a>][<a href="https://github.com/quanhaol/MagicMotion/">Code</a>][<a href="https://quanhaol.github.io/magicmotion-site/">HomePage</a>][<a href="https://mp.weixin.qq.com/s/oGI4NIkVv9xV-pC19LLc3g">é‡å­ä½</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/genrec.png" width="350"> <div class="label">Video Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">GenRec: Unifying Video Generation and Recognition with Diffusion Models </span><br> <span style="font-weight:normal;font-size:16px">Zejia Weng, Xitong Yang</span>, <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2408.15241">Paper</a>]
</span></th></tr></tbody></table>



<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/aid.png" width="350"> <div class="label">Video Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">AID: Adapting Image2Video Diffusion Models for Instruction-based Video Prediction</span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Zejia Weng, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">Technical Report, 2024 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2406.06465">Paper</a>][<a href="https://chenhsing.github.io/AID/">HomePage</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/vidiff.jpg" width="350"> <div class="label">Video Editing</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models</span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Zihao Zhang, Hui Zhang, Han Hu, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">Technical Report, 2024 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2311.18837">Paper</a>][<a href="https://chenhsing.github.io/VIDiff/">HomePage</a>][<a href="https://zhuanlan.zhihu.com/p/670615911">Zhihu</a>]
</span></th></tr></tbody></table>


<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/svformer.jpg" width="350"> <div class="label">Video Recongnition</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">SVFormer: Semi-supervised Video Transformer for Action Recognition </span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Han Hu, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2211.13222">Paper</a>][<a href="https://github.com/ChenHsing/SVFormer">Code</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/panoswin.png" width="350"> <div class="label">3D Understanding</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">PanoSwin: a Pano-style Swin Transformer for Panorama Understanding </span><br> <span style="font-weight:normal;font-size:16px">Zhixin Ling</span>, <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Manliang Cao, Xiangdong Zhou</span></span><br> <span style="font-weight:normal;font-size:16px">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_PanoSwin_A_Pano-Style_Swin_Transformer_for_Panorama_Understanding_CVPR_2023_paper.pdf">Paper</a>][<a href="https://github.com/1069066484/PanoSwinTransformerObjectDetection">Code</a>]
</span></th></tr></tbody></table>


<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/ssp3d.jpg" width="350"> <div class="label">3D Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">Semi-supervised Single-view 3D Reconstruction via Prototype Shape Priors </span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Hengduo Li, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">European Conference on Computer Vision (<strong>ECCV</strong>), 2022 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2209.15383">Paper</a>][<a href="https://github.com/ChenHsing/SSP3D">Code</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/mpcn.jpg" width="350"> <div class="label">3D Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive Network </span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Yijiang Chen, Zhixin Ling, Xiangdong Zhou, Yu Xiang</span></span><br> <span style="font-weight:normal;font-size:16px">European Conference on Computer Vision (<strong>ECCV</strong>), 2022 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2208.00183">Paper</a>][<a href="#">Code</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/csr.jpg" width="350"> <div class="label">Image Retrieval</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">Conditional Stroke Recovery for Fine-Grained Sketch-Based Image Retrieval </span><br> <span style="font-weight:normal;font-size:16px">Zhixin Ling</span>, <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">,Jian Zhou, Xiangdong Zhou</span></span><br> <span style="font-weight:normal;font-size:16px">European Conference on Computer Vision (<strong>ECCV</strong>), 2022 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860708.pdf">Paper</a>][<a href="https://github.com/1069066484/CSR-ECCV2022">Code</a>]
</span></th></tr></tbody></table>


- â€‹**â€‹AdaDiff: Adaptive Step Selection for Fast Diffusionâ€‹**â€‹  
  Hui Zhang, Zuxuan Wu, â€‹**â€‹Zhen Xingâ€‹**â€‹, Jie Shao, Yu-Gang Jiang  
  â€‹**â€‹AAAIâ€‹**â€‹, 2025, [[Paper](https://arxiv.org/abs/2311.14768)]

- â€‹**â€‹Advancing Dark Action Recognition via Modality Fusion and Dark-to-Light Diffusion Modelâ€‹**â€‹  
  Yuxuan Wang, â€‹**â€‹Zhen Xingâ€‹**â€‹, Zuxuan Wu  
  â€‹**â€‹ICASSPâ€‹**â€‹, 2025, [[Paper](https://ieeexplore.ieee.org/abstract/document/10890723)]

- â€‹**â€‹Human2Robot: Learning Robot Actions from Paired Human-Robot Videosâ€‹**â€‹  
  Sicheng Xie, Haidong Cao, Zejia Weng, â€‹**â€‹Zhen Xingâ€‹**â€‹, Shiwei Shen, Jiaqi Leng, Xipeng Qiu, Yanwei Fu, Zuxuan Wu, Yu-Gang Jiang  
  â€‹**â€‹Arxivâ€‹**â€‹, 2025, [[Paper](https://arxiv.org/pdf/2502.16587)]

- â€‹**â€‹Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithmsâ€‹**â€‹  
  Miaosen Zhang, Yixuan Wei, â€‹**â€‹Zhen Xingâ€‹**â€‹, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo  
  â€‹**â€‹NeurIPSâ€‹**â€‹, 2024, [[Paper](https://arxiv.org/pdf/2406.06465.pdf)], [[HomePage](https://chenhsing.github.io/AID/)]

- â€‹**â€‹FDGaussian: Fast Gaussian Splatting via Geometric-aware Diffusion Modelâ€‹**â€‹  
  Qijun Feng, â€‹**â€‹Zhen Xingâ€‹**â€‹, Zuxuan Wu, Yu-Gang Jiang  
  â€‹**â€‹Arxivâ€‹**â€‹, 2024, [[Paper](https://arxiv.org/pdf/2403.10242.pdf)], [[HomePage](https://qjfeng.net/FDGaussian/)]

- â€‹**â€‹TranSFormer: Slow-Fast Transformer for Machine Translationâ€‹**â€‹  
  Bei Li, Yi Jing, Xu Tan, â€‹**â€‹Zhen Xingâ€‹**â€‹, Tong Xiao, Jingbo Zhu  
  â€‹**â€‹ACL (Findings)â€‹**â€‹, 2023, [[Paper](https://arxiv.org/pdf/2305.16982.pdf)]

- â€‹**â€‹Multi-Level Region Matching for Fine-Grained Sketch-Based Image Retrievalâ€‹**â€‹  
  Zhixin Ling, â€‹**â€‹Zhen Xingâ€‹**â€‹, Jiangtong Li, Li Niu  
  â€‹**â€‹ACM MMâ€‹**â€‹, 2022, [[Paper](https://www.jiangtongli.me/publication/mlmr/mlmr.pdf)]

- â€‹**â€‹3D-Augmented Contrastive Knowledge Distillation for Image-based Object Pose Estimationâ€‹**â€‹  
  Zhidan Liu, â€‹**â€‹Zhen Xingâ€‹**â€‹, Xiangdong Zhou, Yijiang Chen, Guichun Zhou  
  â€‹**â€‹ICMRâ€‹**â€‹, 2022, [[Paper](https://arxiv.org/pdf/2206.02531.pdf)]

- â€‹**â€‹CaSS: A Channel-aware Self-supervised Representation Learning Framework for Multivariate Time Series Classificationâ€‹**â€‹  
  Yijiang Chen, Xiangdong Zhou, â€‹**â€‹Zhen Xingâ€‹**â€‹, Zhidan Liu, Minyang Xu  
  â€‹**â€‹DASFFAâ€‹**â€‹, 2022, [[Paper](https://arxiv.org/pdf/2203.04298.pdf)]

- â€‹**â€‹From Coarse to Fine: Hierarchical Structure-aware Video Summarizationâ€‹**â€‹  
  Wenxu Li, Gang Pan, Chen Wang, â€‹**â€‹Zhen Xingâ€‹**â€‹, Zhenjun Han  
  â€‹**â€‹TOMMâ€‹**â€‹, 2022, [[Paper](https://dl.acm.org/doi/abs/10.1145/3485472)]